# 1	TID, PID, TGID

这是最容易混淆的地方，**Linux 内核命名和用户态命名是反过来的**：

| 用户态看到的 | 内核源码字段 | 实际含义 | 类比 |
|-------------|-------------|---------|------|
| **PID** (Process ID) | `tgid` (Thread Group ID) | **线程组 ID**<br>代表整个进程 | 公司营业执照号<br>（整个公司一个） |
| **TID** (Thread ID) | `pid` (Process ID) | **轻量级进程 ID**<br>线程的真实 ID | 员工工号<br>（每人一个，唯一） |
| **TGID** | `tgid` | 同上，PID 的别名 | 同上 |

**记忆口诀**：在内核眼里，**线程就是进程**（轻量级进程），所以内核给线程的 ID 字段名叫 `pid`，但为了兼容 Unix，用户态把它叫做 **TID**。

---

## 1.1	2. 具体例子（数字说明）

假设你启动了一个多线程程序 `app`：

```bash
# 操作系统分配：
主线程（内核任务）：  PID=1000（内核字段），TGID=1000
子线程1（内核任务）： PID=1001（内核字段），TGID=1000  
子线程2（内核任务）： PID=1002（内核字段），TGID=1000
```

### 1.1.1	用户视角（执行命令）：
```bash
$ ps aux | grep app
# 输出：
user    1000  ...  app    ← 你只能看到这一行，因为 ps 默认显示 TGID
```

### 1.1.2	内核视角（/proc 文件系统）：
```bash
$ ls /proc/1000/task/
1000  1001  1002    ← 三个数字！这就是三个内核任务（LWP）

$ cat /proc/1000/task/1001/status | grep ^Tgid
Tgid:   1000        ← 1001号线程属于线程组1000
```

**你现在明白了吗？**
- **1000** 在用户态叫 **PID**（进程号），在内核态叫 **TGID**（线程组号）
- **1001/1002** 在用户态叫 **TID**（线程号），在内核态叫 **PID**（任务号）

---

## 1.2	3. 为什么设计得这么混乱？

### 1.2.1	历史原因：POSIX 兼容性

Unix 传统（如 Solaris、BSD）认为：
- **进程**是资源分配的最小单位（有独立地址空间）
- **线程**是调度的最小单位（共享地址空间）

Linux 的实现革命性在于：
- **线程只是共享资源的进程**（Light Weight Process）
- 没有单独的"线程"数据结构，复用 `task_struct`

**为了兼容旧程序**：
- 当旧程序调用 `getpid()` 时，期望得到"整个进程"的 ID → 返回 `tgid`
- 新程序调用 `gettid()` 时，期望得到"当前线程"的 ID → 返回真实的 `pid`

### 1.2.2	图示对比

```
传统 Unix（如 Solaris）：
进程 A（PID=100）
├── 线程 1（无独立 ID，只称"进程A的线程1"）
└── 线程 2（无独立 ID）

Linux：
进程 A（TGID=100，内核叫 tgid）
├── 线程 1（PID=100，内核 task_struct.pid=100）← 主线程特殊，PID==TGID
├── 线程 2（PID=101，内核 task_struct.pid=101）← 独立任务！
└── 线程 3（PID=102，内核 task_struct.pid=102）← 独立任务！
```

---

## 1.3	4. 编程时的实际体现

### 1.3.1	代码示例（C语言）

```c
#include <stdio.h>
#include <pthread.h>
#include <sys/syscall.h>
#include <unistd.h>

void* worker(void* arg) {
    printf("[子线程] getpid()=%d, gettid()=%ld\n", 
           getpid(),           // 返回 1000（TGID）
           syscall(SYS_gettid) // 返回 1001 或 1002（真实任务ID）
    );
    return NULL;
}

int main() {
    pthread_t t1, t2;
    pthread_create(&t1, NULL, worker, NULL);
    pthread_create(&t2, NULL, worker, NULL);
    
    printf("[主线程] getpid()=%d, gettid()=%ld\n", 
           getpid(),           // 返回 1000（TGID）
           syscall(SYS_gettid) // 返回 1000（主线程PID==TGID）
    );
    
    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    return 0;
}
```

**输出**：
```
[主线程] getpid()=1000, gettid()=1000
[子线程] getpid()=1000, gettid()=1001
[子线程] getpid()=1000, gettid()=1002
```

**关键观察**：
- 三个线程的 `getpid()` 都返回 **1000**（因为它们属于同一个线程组）
- `gettid()` 分别返回 **1000、1001、1002**（主线程特殊，子线程独立）

---

## 1.4	5. 何时用哪个 ID？

### 1.4.1	使用 PID（TGID）的场景：
- **信号发送**：`kill -9 1000` → 影响整个进程组（通常所有线程都会收到）
- **资源统计**：`top`、`ps` 看到的内存占用是进程级别的
- **进程间通信**：管道、共享内存通过 PID 标识

### 1.4.2	使用 TID（真实 pid）的场景：
- **CPU 绑定**：`taskset -c 0 -p 1001`（把特定线程绑到 CPU0）
- **调试特定线程**：`gdb -p 1001`（ attach 到卡死的线程）
- **内核日志**：`dmesg` 里看到的 segfault 地址对应的是 TID
- **futex 同步**：用户态锁记录的持有者 ID 是 TID（因为同进程的不同线程需要区分）

---

## 1.5	6. 与其他操作系统对比（加深理解）

| 系统 | 概念 | 特点 |
|------|------|------|
| **Windows** | Process ID / Thread ID | 严格区分，线程 ID 是独立命名空间，与进程 ID 无数值关系 |
| **Linux** | PID / TID | 位于同一命名空间（都是全局递增整数），**线程就是特殊进程** |
| **macOS** | Mach Port | 使用端口（类似句柄）标识，更抽象 |

**Linux 的独特之处**：你可以对单个线程发送信号（`tkill(2)`），就像对待独立进程一样，这是其他系统做不到的。

---

## 1.6	总结

- **TGID**（Thread Group ID）：只是为了兼容旧 Unix 标准而存在的概念，**用户态叫它 PID**
- **PID**（内核字段）：Linux 内核中**真实的任务 ID**，**用户态叫它 TID**
- **设计哲学**：Linux 认为线程就是共享地址空间的进程，因此每个线程都有独立的 `task_struct` 和独立的调度属性（可以单独绑核、单独设优先级）

现在再看这张图应该就清晰了：

```
用户命令：ps aux
显示：PID=1000 → 实际读取的是内核的 tgid 字段

用户命令：cat /proc/1000/task/1001/status
显示：Tgid: 1000 → 这是内核的 tgid 字段
      Pid:  1001 → 这是内核的 pid 字段（用户叫 TID）
```

# 2	Linux到底是运行在一个核上还是运行在多个核上共同协商调度？
 **Linux 同时运行在多个核上，且没有"主核"概念。**

每个 CPU 核心都运行着独立的调度器实例，自主决策本地任务的切换，同时通过**负载均衡（Load Balancing）**机制与其他核心协商，确保全局任务分布最优。

---

## 2.1	1. 架构：对称多处理（SMP）

Linux 采用 **SMP（Symmetric Multi-Processing）** 架构：

```
CPU0                    CPU1                    CPU2       
├─ 调度器（CFS）        ├─ 调度器（CFS）        ├─ 调度器（CFS）     
├─ 运行队列（rq0）      ├─ 运行队列（rq1）      ├─ 运行队列（rq2）    
├─ 本地定时器中断       ├─ 本地定时器中断       ├─ 本地定时器中断     
└─ tick_schedule()      └─ tick_schedule()      └─ tick_schedule()   
```

**关键特性**：
- **每个核心有独立的运行队列**（`struct rq`），无需全局锁竞争
- **每个核心独立触发调度**：时钟中断（tick）在每个 CPU 本地产生，各自调用 `schedule()`
- **无中心协调者**：CPU0 不会告诉 CPU1 "你该运行哪个任务"

---

## 2.2	2. 调度流程：每个核心自己做主

当 CPU0 上的线程 A 时间片用完时：

```c
// 发生在 CPU0 本地
schedule() {
    // 1. 只操作 CPU0 的本地运行队列（rq0），无需通知其他核
    next = pick_next_task(rq0);  // 从本地红黑树选任务
    
    // 2. 上下文切换到线程 B（同样在 CPU0 上运行）
    context_switch(prev, next);
}
```

**此时 CPU1/2/3 完全不知道 CPU0 发生了调度**，它们也在并行执行自己的调度循环。

---

## 2.3	3. 协商机制：负载均衡（Load Balancing）

虽然各自独立调度，但为了避免"CPU0 忙死、CPU3 闲死"的情况，内核通过 **_sched domain_** 层级结构定期协商：

### 2.3.1	触发条件
- **周期性均衡**：每隔 `HZ`（通常是 250ms）检查一次
- **空闲均衡**：当某核心空闲时，立即从忙碌核心"偷"任务
- **唤醒均衡**：新任务唤醒时，选择最空闲的 CPU 运行（而非必然在原 CPU）

### 2.3.2	核心算法（简化）
```c
// CPU0 的负载均衡器发现本地队列过长
load_balance() {
    // 1. 找到最空闲的 CPU（比如 CPU3）
    busiest = find_busiest_queue();  // 可能是 CPU0 自己
    target = find_idlest_queue();    // 可能是 CPU3
    
    // 2. 从忙碌队列迁移任务到目标队列
    move_tasks(busiest, target);
}
```

**注意**：这是**分布式协商**，不是主从架构。每个 CPU 都可能成为"发起均衡"的一方。

---

## 2.4	4. 内核线程的分布

Linux 内核本身的后台任务也是分布在所有核上的：

```bash
# 查看内核线程分布
ps -eo psr,comm | grep ksoftirqd
  0 ksoftirqd/0
  1 ksoftirqd/1
  2 ksoftirqd/2
  3 ksoftirqd/3

ps -eo psr,comm | grep migration
  0 migration/0   # 负责 CPU0 上的任务迁移
  1 migration/1   # 负责 CPU1 上的任务迁移
```

- **`ksoftirqd/N`**：每个核有一个，处理本核的软中断
- **`migration/N`**：每个核有一个，执行本核的任务迁移操作（当需要将任务"推"到其他核时）

---

## 2.5	5. 全局状态的同步

虽然调度是分布式的，但某些全局操作需要同步：

| 机制 | 用途 | 优化 |
|------|------|------|
| **per-CPU rq_lock** | 保护每个运行队列 | 无竞争时无锁开销 |
| **RCU** | 读取任务结构（task_struct） | 读操作几乎零开销 |
| **stop_machine** | CPU 热插拔、大内核锁淘汰 | 极少数情况下暂停其他核 |

**现代优化（Linux 2.6+ 至今）**：
- 淘汰了 **Big Kernel Lock (BKL)**，避免了"一个核进入内核，其他核只能等待"的噩梦
- 采用 **per-CPU 变量**：每个核有自己的计数器副本，避免缓存行（Cache Line） bouncing

---

## 2.6	总结

- **运行方式**：Linux 内核镜像同时加载到所有核的内存中，**每个核独立执行内核代码**
- **调度决策**：完全分布式，**N 个核就有 N 个调度器实例**
- **协调机制**：通过**负载均衡算法**周期性协商任务迁移，而非集中式调度
- **类比**：像是一个没有总经理的**合伙人制度**——每个合伙人（CPU）自己接活儿、干活儿，但定期开会（load_balance）调整工作量，确保没人累死也没人闲死。

这也是为什么 Linux 能良好扩展到 1024+ 核心：没有单点瓶颈，每个核心都是自治的调度单元。

# 3	ps aux 中的VSZ和RSS是什么意思
 `ps aux` 中的 **VSZ** 和 **RSS** 是衡量进程内存占用的两个维度，分别对应**虚拟地址空间**和**物理内存实际占用**。

---

## 3.1	1. 基本定义

| 指标 | 全称 | 含义 | 单位 |
|------|------|------|------|
| **VSZ** | Virtual Memory Size | **虚拟内存大小**<br>进程可访问的地址空间总量 | KB |
| **RSS** | Resident Set Size | **常驻内存集**<br>实际驻留在物理内存（RAM）中的页数 | KB |

**核心区别**：VSZ 是"进程看到的内存"，RSS 是"物理内存实际承载的进程数据"。

---

## 3.2	2. VSZ：虚拟地址空间

### 3.2.1	包含内容
- **代码段**：可执行文件本身的映射
- **堆（Heap）**：malloc/new 分配的内存（**即使尚未实际使用**）
- **栈（Stack）**：函数调用栈
- **内存映射文件**：mmap 映射的文件、共享库（.so）
- **共享内存**：System V 共享内存、POSIX 共享内存

### 3.2.2	关键特性
**延迟分配（Demand Paging）机制**：
```c
// 分配 1GB 内存，但还没写入数据
char *p = malloc(1024 * 1024 * 1024);  // VSZ 立即 +1GB
// 此时 RSS 不变，因为没有物理页被分配

memset(p, 0, 4096);  // 第一次写入（触发 Page Fault）
// 此时 RSS 仅增加 4KB（一页），其余仍为虚拟地址
```

**结论**：VSZ 可以远大于物理内存总量（如 32 位系统进程 VSZ 可达 3GB，64 位系统可达 128TB 地址空间），**不代表实际内存压力**。

---

## 3.3	3. RSS：物理内存占用

### 3.3.1	包含内容
- **实际分配的物理页**：已写入数据的堆、栈、代码段
- **共享库**：libc.so 等在物理内存中的页（**会被多个进程重复计算**）
- **匿名页**：堆、栈等私有数据

### 3.3.2	不包含内容
- **交换出去的内存（Swap）**：被换出到磁盘的页
- **未分配的虚拟页**：malloc 但未 touch 的内存
- **文件缓存（Page Cache）**：mmap 文件但未修改的部分（实际在系统缓存，不算入进程 RSS）

---

## 3.4	4. 为什么 VSZ 远大于 RSS？

典型场景分析：

```bash
$ ps aux | grep python
USER   PID %CPU %MEM    VSZ   RSS TTY   STAT START   COMMAND
root  1234  0.5  1.2  85000  5000 ?     S    10:00   python app.py
```

**VSZ=85MB vs RSS=5MB 的差距来源**：

| 来源        | 说明                                          | 占比         |
| --------- | ------------------------------------------- | ---------- |
| **共享库**   | libc、libpython 等映射到地址空间，但实际物理页可能被多个进程共享     | ~2-20MB    |
| **预分配内存** | Python 预分配的堆内存池（pymalloc），VSZ 计入，但尚未使用      | ~10-50MB   |
| **线程栈预留** | 每个线程栈默认 8MB（VSZ），实际只用几 KB（RSS）              | 8MB × N 线程 |
| **内存映射**  | mmap 的大文件（如日志、数据库），VSZ 包含文件大小，但 RSS 仅缓存活跃部分 | 可变         |

---

## 3.5	5. 共享内存的特殊性（陷阱）

**重要误区**：RSS **会重复计算共享内存**。

```bash
# 两个进程共享 100MB 的库
进程 A：RSS = 50MB（私有）+ 100MB（共享）= 150MB
进程 B：RSS = 50MB（私有）+ 100MB（共享）= 150MB

# 实际物理内存占用：50 + 50 + 100 = 200MB
# 但 ps 看总和会是 300MB（共享部分被算两次）
```

**查看真实物理占用**（剔除重复计算）：
```bash
# PSS（Proportional Set Size）：按比例分摊共享内存
cat /proc/1234/smaps | grep Pss | awk '{sum+=$2} END {print sum}'

# 或使用 smem 工具
smem -P python
```

---

## 3.6	6. 实际应用场景

### 3.6.1	判断内存泄漏
- **VSZ 持续增长，RSS 不变**：可能是大量 malloc 但未使用（地址空间耗尽风险）
- **VSZ 和 RSS 同步增长**：真实内存泄漏（物理内存压力）
- **RSS 缓慢增长，VSZ 稳定**：可能是正常的缓存积累（如读取大文件进入 Page Cache）

### 3.6.2	OOM 风险评估
OOM Killer 触发时主要看 **RSS + Swap 占用**，但 VSZ 过大也会导致地址空间耗尽（64 位系统极少见，32 位系统常见）。

### 3.6.3	容器资源限制
Kubernetes/Docker 的 `memory.limit_in_bytes` 限制的是 **RSS + Cache + Swap**（cgroups v1）或 **完整内存占用**（cgroups v2），不是 VSZ。

---

## 3.7	7. 总结记忆

| 场景             | 观察指标           | 原因                    |
| -------------- | -------------- | --------------------- |
| **系统内存是否够用**   | RSS（结合 PSS 更佳） | 反映真实物理压力              |
| **进程地址空间是否耗尽** | VSZ            | 32 位程序接近 3GB 会崩溃      |
| **是否存在内存泄漏**   | VSZ & RSS 趋势   | 两者背离说明延迟分配，同步增长说明真实泄漏 |
| **共享库内存优化**    | RSS 高但 PSS 低   | 说明大量使用共享内存，实际物理占用不高   |

**一句话**：**VSZ 是"画饼"，RSS 是"真吃"**。看内存压力盯 RSS，看地址空间限制盯 VSZ。

# 4	socket、消息队列、FIFO的区别

| 特性       | **Socket**        | **消息队列**       | **FIFO（命名管道）**    |
| -------- | ----------------- | -------------- | ----------------- |
| **通信范围** | 同一机器 / **跨网络**    | 同一机器           | 同一机器              |
| **通信模式** | 字节流（TCP）或数据报（UDP） | 结构化消息          | 字节流               |
| **持久性**  | 无（连接断开即消失）        | **内核持久**（重启丢失） | 无（无进程使用即消失）       |
| **双向通信** | ✅ 全双工             | ✅ 可双向          | ❌ 单向（需两个FIFO实现双向） |
| **数据边界** | TCP无边界 / UDP有边界   | **有消息边界**      | 无边界（字节流）          |
| **主要用途** | 网络通信、本地高速IPC      | 进程间异步消息传递      | 简单本地IPC、shell脚本   |
